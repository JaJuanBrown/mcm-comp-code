---
title: "MCM Project"
format: html
editor: visual
---

## Importing Libraries

```{r include=FALSE}
#Installing Packages
library(janitor)
library(readxl)
library(tidyverse)
library(tidymodels)
library("olsrr")
library("doParallel")
library(readr)
library(MASS)
library(rpart)
library(rpart.plot)
library(caret)
library(gbm)
library(xgboost)
library(stringr)
all_cores<-parallel::detectCores()
cl<- makePSOCKcluster(all_cores)
registerDoParallel(cl)
library(AICcmodavg)
install.packages("udpipe")
library(udpipe)
```

## Importing Data

```{r}
data<-read_excel("Problem_C_Data_Wordle.xlsx", skip=1) %>% 
  clean_names()

#Removing words that arent 5 letters
data<-data %>% 
  filter(word != "rprobe") %>% 
  filter(word != "clen") %>%
  filter(word != "tash")  %>% 
  filter(word != "marxh")
```

## Exploratory Analysis

```{r}
#Visualizing Relationships

#Comparing Date to number of people that participate
data %>% 
  ggplot(aes(x=date, y=number_of_reported_results))+
  geom_point()

#Are people getting better
data %>% 
  ggplot(aes(x=date, y=data$x7_or_more_tries_x))+
  geom_point()

#Are more people trying hard mode
data %>% 
  ggplot(aes(x=date, y=data$number_in_hard_mode))+
  geom_point()

#First attempt Guesses over time
data %>% 
  ggplot(aes(x=date, y=data$x1_try))+
  geom_point()



```

## Problem 1: Predicting number of players

This needs to be a regression model. This data is in no way linear so the first attempt is to create a nonlinear regression method.

```{r}
#Creating Data splits
set.seed(1234)

split<-initial_split(data, prop = .9)
training_data<-training(split)
test_data<-testing(split)

#The Future Date we want to predict the participants for
future_date<-data_frame(date = as.Date(x = "2023/3/1"),
                        contest_number = 620)

```

## Basic Decision Tree

```{r}
#Tuned Model
tuned_reg_tree<-decision_tree(mode = "regression", engine = "rpart", cost_complexity = 0.0005623413, tree_depth = 11, min_n = 2)
#Testing Model
reg_tree_wf<-workflow() %>% 
  add_model(tuned_reg_tree) %>% 
  add_formula(number_of_reported_results~date+contest_number) %>% 
  fit(training_data)
```

## Boosted Tree

```{r}
#Updating Workflow with Tuned Model and Fitting it to Training Data

tuned_reg_xgb<-boost_tree(mode = "regression", engine = "xgboost",mtry = 2, trees = 1000, min_n = 13, tree_depth = 6, learn_rate = 0.01872842, loss_reduction = 1.647096, sample_size = 0.9797893)

```

## Support Vector Machine Regression

```{r}
#Updating Workflow with Tuned Model and Fitting it to Training Data

tuned_svm<-svm_rbf(mode = "regression", engine = "kernlab", cost = 32, rbf_sigma = 1, margin = 0.1)

```

## Comparing Regressions

## Assessing Word Difficulty

```{r}
difficulty<-c()
for (i in 1:nrow(data)){
  current_row=data[i,]
  #Determining the difficulty of this word
  if (((current_row$x1_try + current_row$x2_tries + current_row$x3_tries) >
    (current_row$x3_tries + current_row$x4_tries)) && ((current_row$x1_try + current_row$x2_tries + current_row$x3_tries) >
    (current_row$x4_tries + current_row$x5_tries)) && ((current_row$x1_try + current_row$x2_tries + current_row$x3_tries) >
    (current_row$x5_tries + current_row$x6_tries)) && ((current_row$x1_try + current_row$x2_tries + current_row$x3_tries) >
    (current_row$x6_tries + current_row$x7_or_more_tries_x))){
    difficulty[i]<- "Very Easy"
  }else if (((current_row$x3_tries + current_row$x4_tries) >
    (current_row$x1_try + current_row$x2_tries + current_row$x3_tries)) && ((current_row$x3_tries + current_row$x4_tries) >
    (current_row$x4_tries + current_row$x5_tries)) && ((current_row$x3_tries + current_row$x4_tries) >
    (current_row$x5_tries + current_row$x6_tries)) && ((current_row$x3_tries + current_row$x4_tries) >
    (current_row$x6_tries + current_row$x7_or_more_tries_x))){
    difficulty[i]<- "Easy"
  }else if (((current_row$x4_tries + current_row$x5_tries) >
    (current_row$x1_try + current_row$x2_tries + current_row$x3_tries))== TRUE && ((current_row$x4_tries + current_row$x5_tries) >
    (current_row$x3_tries + current_row$x4_tries))== TRUE && ((current_row$x4_tries + current_row$x5_tries) >
    (current_row$x5_tries + current_row$x6_tries))== TRUE && ((current_row$x4_tries + current_row$x5_tries) >
    (current_row$x6_tries + current_row$x7_or_more_tries_x))== TRUE){
    difficulty[i]= "Medium"
  }else if (((current_row$x5_tries + current_row$x6_tries) >
    (current_row$x1_try + current_row$x2_tries + current_row$x3_tries)) && ((current_row$x5_tries + current_row$x5_tries) >
    (current_row$x3_tries + current_row$x4_tries)) && ((current_row$x5_tries + current_row$x6_tries) >
    (current_row$x4_tries + current_row$x5_tries)) && ((current_row$x5_tries + current_row$x6_tries) >
    (current_row$x6_tries + current_row$x7_or_more_tries_x))){
    difficulty[i]<- "Hard"
  }else if (((current_row$x6_tries + current_row$x7_or_more_tries_x) >
    (current_row$x1_try + current_row$x2_tries + current_row$x3_tries)) && ((current_row$x6_tries + current_row$x7_or_more_tries_x) >
    (current_row$x3_tries + current_row$x4_tries)) && ((current_row$x6_tries + current_row$x7_or_more_tries_x) >
    (current_row$x4_tries + current_row$x5_tries)) && ((current_row$x6_tries + current_row$x7_or_more_tries_x) >
    (current_row$x5_tries + current_row$x6_tries))){
    difficulty[i]<- "Very Hard"
  }else {difficulty[i]<-NA}
      
}
difficulty<-as_factor(difficulty)

data$difficulty<-difficulty

```

## Analyzing Aspects of the Words

```{r}
#Vowel Counting Function
get_vowel_count <- function(phrase) {
  counter <- 0
  for (i in unlist(strsplit(phrase, ""))) {
    if ( i %in% c("a", "e", "i", "o", "u")) {
      counter <- counter + 1 
    }   
  }
  counter
}

#Counting Vowels for every word
vowels<-c()
counter<-0
for (i in 1:nrow(data)){
  current_word = data$word[i]
  #print(current_word)
  vowels[i]<- get_vowel_count(current_word)
  #print(get_vowel_count(current_word))
}

data$vowel_count<-vowels

#Counting Number of consonants
data$consonant_count<-5-data$vowel_count

#Creating tricky letter function
get_tricky_letter_count <- function(phrase) {
  counter <- 0
  for (i in unlist(strsplit(phrase, ""))) {
    if ( i %in% c("r", "u", "w", "z", "x", "y")) {
      counter <- counter + 1 
    }   
  }
  counter
}

#Counting tricky letters
tricky_letters<-c()
for (i in 1:nrow(data)){
  current_word = data$word[i]
  #print(current_word)
  tricky_letters[i]<- get_tricky_letter_count(current_word)
  #print(get_tricky_letter_count(current_word))
}

data$tricky_letters<-tricky_letters

#Creating a function for counting letters
get_letter_count <- function(phrase) {
  letters<- as.vector(strsplit(phrase, ""))
  counter <- c()
  for (i in letters) {
 count <- str_count(phrase, i)
 counter[i]<-count
  }
  max(counter) 
}
#Counting the number of occurances of each letter
max_repeats<-c()
for (i in 1:nrow(data)){
  current_word = data$word[i]
  #print(current_word)
  max_repeats[i]<- get_letter_count(current_word)
  #print(get_tricky_letter_count(current_word))
}

data$max_repeats<-max_repeats
```

```{r}
#Checking which words dont have a difficulty
data$difficulty[data$word == "elder"]<- "Hard"
data$difficulty[data$word == "naÃ¯ve" | data$word == "spell" | data$word == "berth"| data$word == "retro"| data$word == "girth"| data$word == "cater"| data$word == "mince"]<-"Medium"

```

## EDA for Comparing Difficulties

```{r}
data %>% 
  group_by(difficulty) %>% 
  count
#Assess the vowel count over different difficulties
data %>% 
  ggplot(aes(x=difficulty, y=vowel_count))+
  geom_col(aes(fill = difficulty))

#Assessing Consonant Count Across Difficulties
data %>% 
  ggplot(aes(x=difficulty, y=consonant_count))+
  geom_col(aes(fill = difficulty))

#Assessing Tricky Letter Count Across Difficulties
data %>% 
  ggplot(aes(x=difficulty, y=tricky_letters))+
  geom_col(aes(fill = difficulty))

#Assessing Letter Repetition Across Difficulties
data %>% 
  ggplot(aes(x=difficulty, y=max_repeats))+
  geom_col(aes(fill = difficulty))

#Assessing Difficulty Over Time
data %>%
  ggplot(aes(x=date, y=difficulty))+
  geom_col(aes(fill = difficulty))

#Detecting parts of speech

udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(file = udmodel$file_model)


wordle_pos <- udpipe_annotate(udmodel, 
                              data$word)
wordle_pos <- as.data.frame(wordle_pos)

wordle_pos %>% dplyr::select(token, upos)

data$pos <- wordle_pos$upos
data$xpos<-wordle_pos$xpos
data$feats<- wordle_pos$feats


#Finding Average  vowel count, consonant count, tricky letter count, and double letter count for each difficulty


data %>% 
  group_by(difficulty) %>% 
  summarise(avg_vowels= mean(vowel_count),
            .groups = "drop") %>% 
  arrange(avg_vowels)

data %>% 
  group_by(difficulty) %>% 
  summarise(avg_consonants= mean(consonant_count),
            .groups = "drop") %>% 
  arrange(avg_consonants)

data %>% 
  group_by(difficulty) %>% 
  summarise(avg_tricky= mean(tricky_letters),
            .groups = "drop") %>% 
  arrange(avg_tricky)

data %>% 
  group_by(difficulty) %>% 
  summarise(avg_repeats= mean(max_repeats),
            .groups = "drop") %>% 
  arrange(avg_repeats)

```

## Decision for Classifying Difficulty

```{r}
#Creating Data splits
set.seed(1234)

split<-initial_split(data, prop = .9)
training_data<-training(split)
test_data<-testing(split)

#The Future Date we want to predict the participants for
future_word<-tibble(word = "eerie", vowel_count = 4, consonant_count =1, tricky_letters = 0, max_repeats = 3)

```

```{r}
#Decision Tree #
tree_model<-decision_tree(mode = "classification",
                          engine = "rpart",
                          cost_complexity = tune(),
                          tree_depth = tune(),
                          min_n = tune())

tree_wf<-workflow() %>% 
  add_model(tree_model) %>% 
  add_formula(difficulty~vowel_count+consonant_count+tricky_letters+max_repeats)

folds<-vfold_cv(training_data)

tree_tuning_grid<-grid_regular(cost_complexity(),
                               tree_depth(),
                               min_n(),
                               levels = 5)

tree_res<-tune_grid(tree_wf,
                    resamples = folds,
            grid = tree_tuning_grid)

best_tree_params<-tree_res %>% select_best("accuracy")
```

```{r}
#Accessing model Accuracy
tuned_tree<-finalize_model(tree_model, best_tree_params)
tree_model<-decision_tree(mode = "classification",
                          engine = "rpart",
                          tree_depth = 4,
                          min_n = 11,
                          cost_complexity = 1e-10)
tree_wf<-tree_wf %>%
  update_model(tree_model) %>% 
  fit(training_data)

tree_preds<-predict(tree_wf, test_data)

#Assessing where the model overpredicted
tree_preds %>% 
  group_by(.pred_class) %>% 
  count()
test_data %>% 
  group_by(difficulty) %>% 
  count()
tree_acc<-mean(tree_preds$.pred_class == test_data$difficulty)

#Predicting "eerie" difficulty
predict(tree_wf, future_word)
```

## Boosted Tree Classification

```{r}
# Boosted Tree #

xgb_classification<-boost_tree(mode = "classification",
                               engine = "xgboost",
                               mtry = tune(),
                               trees = 1000,
                               min_n = tune(),
                               tree_depth = tune(),
                               learn_rate = tune(),
                               loss_reduction = tune(),
                               sample_size = tune(),
                               stop_iter = tune())

xgb_wf<- workflow() %>% 
  add_model(xgb_classification) %>% 
  add_formula(difficulty~vowel_count+consonant_count+tricky_letters+max_repeats)

folds<-vfold_cv(training_data)
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), training_data),
  learn_rate(),
  stop_iter(),
  size = 30
)

xgb_res<- tune_grid(xgb_wf,
                    resample = folds,
            grid = xgb_grid,
            control = control_grid(save_pred = TRUE))

best_xgb_params<-xgb_res %>% select_best("accuracy")
```

```{r}
#Accessing model Accuracy
#This is tuned
tuned_xgb<-boost_tree(mode = "classification",
                               engine = "xgboost",
                               mtry = 9,
                               trees = 1000,
                               min_n = 12,
                               tree_depth = 1,
                               learn_rate = 0.01458431,
                               loss_reduction = 2.571962e-08,
                               sample_size = 0.5142188,
                               stop_iter = 9)

tuned_xgb<-finalize_model(xgb_classification, best_xgb_params)
xgb_wf<-xgb_wf %>%
  update_model(tuned_xgb) %>% 
  fit(training_data)

xgb_preds<-predict(xgb_wf, test_data)

#Assessing where the model overpredicted
xgb_preds %>% 
  group_by(.pred_class) %>% 
  count()
test_data %>% 
  group_by(difficulty) %>% 
  count()
xgb_acc<-mean(xgb_preds$.pred_class == test_data$difficulty)

#Predicting "eerie" difficulty
predict(xgb_wf, future_word)
```
